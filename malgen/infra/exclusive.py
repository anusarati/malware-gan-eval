"""Filters out all images that are in multiple malware families"""

from collections import namedtuple
import json
import sqlite3
from malgen.infra.constants import CATEGORIES, COLUMNS, OTHER_METADATA
import pyarrow as pa
import pyarrow.parquet as pq
import requests
import logging
from tqdm.contrib.concurrent import thread_map
from tqdm import tqdm
from threading import Lock


def save_exclusive():
    con = sqlite3.connect("meta.db")

    hashes = []
    labels = []

    Row = namedtuple("Row", COLUMNS)
    con.row_factory = lambda _, data: Row(*data)
    cur = con.cursor()
    res = cur.execute(f"SELECT * FROM meta WHERE is_malware >= 1;")
    while row := res.fetchone():
        if row.is_malware:
            categories = row[len(OTHER_METADATA) :]
            if sum(categories) == 1:
                hashes.append(row.sha256)
                labels.append(categories.index(1))

    table = pa.Table.from_arrays([hashes, labels], names=["hash", "label"])
    pq.write_table(table, "exclusive.parquet")
    print(table)
    con.close()


def save_retrieval_list(num_per_class):
    table = pq.read_table("exclusive.parquet").to_pandas()
    families = thread_map(
        lambda i: table.query(f"label == {i}"), range(len(CATEGORIES))
    )
    frequencies = [family.shape[0] for family in families]
    large_family_indices = [
        i for i in range(len(CATEGORIES)) if frequencies[i] >= num_per_class
    ]
    print(large_family_indices)
    with open("large_families.json", "w") as f:
        json.dump(large_family_indices, f)
    large_families = [families[i] for i in large_family_indices]

    shuffled = [family.sample(frac=1) for family in large_families]
    logger = logging.getLogger()
    keep = set()
    with open("get.txt", "w") as f:
        for family, idx in zip(shuffled, large_family_indices):
            hashes = family["hash"].values
            n_found = 0
            lock = Lock()
            def check_available(_hash):
                nonlocal n_found
                if n_found < num_per_class:
                    url = f"http://sorel-20m.s3.amazonaws.com/09-DEC-2020/binaries/{_hash}"
                    if requests.head(url).ok:
                        with lock:
                            keep.add(_hash)
                            print(url, file=f)
                            n_found += 1
            
            thread_map(check_available, hashes)

            if n_found < num_per_class:
                logger.warning(f"{CATEGORIES[idx]} doesn't have enough available samples")
    keep = table.query("sha256 in @keep")
    keep.to_parquet("keep.parquet")
