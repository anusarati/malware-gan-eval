import torchmetrics
import torch
import argparse
from pathlib import Path
import pickle
from malgen.bin_vis.reduce import reduce
from malgen.infra.imagedata import ImageDataModule
from malgen.infra.classifier import Classifier
from tqdm import tqdm


def torch_fidelity_metrics(generated, real):
    import torch_fidelity

    return torch_fidelity.calculate_metrics(
        input1=generated,
        input2=real,
        cuda=True,
        isc=True,
        fid=True,
        kid=True,
        prc=True,
        ppl=True,
        verbose=False,
    )


@torch.inference_mode()
def classification_comparison(
    classifier: Classifier, dataloader, one_hot=True, outdir="."
):
    dataset_predictions = []
    dataset_labels = []
    for images, labels in tqdm(dataloader):
        # each image should have a binary label vector
        # one-hot should only be False for the control (real vs real)
        predictions = classifier(images).sigmoid()

        # if it's one-hot, every prediction other than for that label
        # will be ignored
        if one_hot:
            predictions *= labels
            predictions = predictions.max(-1).values
            # because we only keep the positive label, ROC is not as useful
            labels = torch.ones(labels.shape[0])

        dataset_predictions.append(predictions)
        dataset_labels.append(labels)
    dataset_predictions = torch.cat(dataset_predictions)
    dataset_labels = torch.cat(dataset_labels)
    return dataset_predictions, dataset_labels


def classification_metrics(predictions, labels):
    accuracy = torchmetrics.Accuracy(task="binary")
    f1 = torchmetrics.F1Score(task="binary")
    precision = torchmetrics.Precision(task="binary")
    recall = torchmetrics.Recall(task="binary")
    return {
        "accuracy": accuracy(predictions, labels),
        "f1": f1(predictions, labels),
        "precision": precision(predictions, labels),
        "recall": recall(predictions, labels),
    }


def all_metrics(trainset, valset, genset, classifier_t, classifier_v, classifier_g):
    metrics = torch_fidelity_metrics(trainset, genset)

    # GAN-test
    gan_test = classification_comparison(classifier_t, genset)
    metrics["GAN-test"] = gan_test

    # Neural network divergence
    nnd = classification_comparison(classifier_v, genset)
    metrics["Neural network divergence"] = nnd

    # Compute control outside (no relation to genset)

    # GAN-train
    gan_train = classification_comparison(classifier_g, valset)
    metrics["GAN-train"] = gan_train

    return metrics


# train on training set
# test on validation set (control)
# train on validation set


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("-tr", "--train_dir")
    parser.add_argument("-va", "--va_dir")
    parser.add_argument("-ge", "--gen_dir")
    parser.add_argument("-od", "--outdir", default=".")
    subparsers = parser.add_subparsers(dest="sub")
    tfm_parser = subparsers.add_parser("tfm", help="Get Torch fidelity metrics")

    clsi_parser = subparsers.add_parser(
        "clsi", help="Prepare predictions for Pytorch classification metrics"
    )
    clsi_parser.add_argument(
        "-cls", "--classifier", help="Path to classifier checkpoint", required=True
    )
    clsi_parser.add_argument(
        "-b", "--batch_size", help="Batch size", default=16, type=int
    )
    clsi_parser.add_argument(
        "-m",
        "--multi_label",
        help="Whether to evaluate all labels per sample for classification metrics",
        action=argparse.BooleanOptionalAction,
    )
    clsi_parser.add_argument(
        "-l", "--label_type", choices=["pkl", "dir"], default="dir"
    )
    clsi_parser.add_argument("-w", "--world_size", type=int, default=1)
    clsi_parser.add_argument("-rank", type=int)

    clsm_parser = subparsers.add_parser(
        "clsm", help="Get Pytorch classification metrics"
    )

    args = parser.parse_args()
    print(args)
    outdir = Path(args.outdir)
    if args.sub == "tfm":
        metrics = torch_fidelity_metrics(args.gen_dir, args.train_dir)
        print(metrics)
        with open(f"tfm-{Path(args.gen_dir).name}.pkl") as f:
            pickle.dump(metrics, f)
    elif args.sub == "clsi":
        datamod = ImageDataModule(
            val_dir=args.va_dir, label_type=args.label_type, bucket=False
        )
        datamod.setup("test", rank=args.rank, world_size=args.world_size)
        dataloader = datamod.val_dataloader()
        clsi = Classifier.load_from_checkpoint(args.classifier)
        predictions, labels = classification_comparison(
            clsi, dataloader, not args.multi_label, outdir=args.outdir
        )
        torch.save(predictions, outdir / f"_pred-{args.rank}.pt")
        torch.save(labels, outdir / f"_labels-{args.rank}.pt")
    elif args.sub == "clsm":

        def rfun(l, x):
            l.append(x)
            return l

        predictions_path = outdir / "predictions.pt"
        labels_path = outdir / "labels.pt"

        if predictions_path.is_file():
            predictions = torch.load(predictions_path)
        else:
            predictions = torch.cat(reduce(outdir, rfun, [], name="pred"))
            torch.save(predictions, outdir / "predictions.pt")
        if labels_path.is_file():
            labels = torch.load(labels_path)
        else:
            labels = torch.cat(reduce(outdir, rfun, [], name="labels"))
            torch.save(labels, outdir / "labels.pt")

        metrics = classification_metrics(predictions, labels)
        print(metrics)
        with open(outdir / f"metrics", "w") as f:
            print(metrics, file=f)
    else:
        print(f"Command {args.sub} unrecognized")
        parser.print_help()
