import lightning as L
import os
from pathlib import Path
import pickle
from lightning.pytorch.utilities.types import TRAIN_DATALOADERS
import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, Sampler, default_collate
import torchvision
from torchvision.transforms import v2
import functools
from math import ceil
import random
from constants import CATEGORIES


def get_image_paths(dir):
    is_image = lambda path: Path(path).suffix in {".pt", ".png", ".jpg", ".jpeg"}
    paths = [
        Path(dirpath) / filename
        for dirpath, _, filenames in os.walk(dir)
        for filename in filenames
        if is_image(filename)
    ]
    return paths


class ImageDataset(Dataset):
    def __init__(self, paths, labels, transform=lambda x: x) -> None:
        super().__init__()
        self.labels = labels
        self.paths = paths
        self.transform = transform

    def __len__(self):
        return len(self.paths)

    def load_image(self, path):
        path = Path(path)
        if path.suffix == ".pt":
            return torch.load(path)
        img = torchvision.io.read_image(path)
        if img.shape[-1] > 512:
            img = v2.functional.resize(
                img,
                (512, 512),
                interpolation=torchvision.transforms.InterpolationMode.BICUBIC,
            )
        elif img.shape[-1] < 64:
            img = v2.functional.resize(
                img,
                (64, 64),
                interpolation=torchvision.transforms.InterpolationMode.BICUBIC,
            )
        return img.float() / 255

    @functools.lru_cache(maxsize=65536)
    def __getitem__(self, index):
        path = self.paths[index]
        img = self.load_image(path)
        img = self.transform(img)
        if self.labels:
            label = self.labels[path.name]
            return img, label
        return img


class ImageBuckets(Dataset):
    def __init__(self, data: ImageDataset, batch_size, buckets):
        self.data = data
        self.batch_size = batch_size
        self.buckets = buckets
        self.index = {path.name: i for i, path in enumerate(data.paths)}
        self.gen_batches()

    def gen_batches(self):
        batches = []
        for paths in self.buckets.values():
            random.shuffle(paths)
            for i in range(ceil(len(paths) / self.batch_size)):
                batch = paths[self.batch_size * i : self.batch_size * (i + 1)]
                batch = [self.index[path] for path in batch]
                batches.append(batch)
        random.shuffle(batches)
        self.batches = batches

    def __len__(self):
        return len(self.batches)

    def __getitem__(self, index):
        batch_indices = self.batches[index]
        return default_collate([self.data[i] for i in batch_indices])


class ImageDataModule(L.LightningDataModule):
    def __init__(
        self,
        train_dir=None,
        val_dir=None,
        label_type="pkl",
        bucket=True,
        batch_size=16,
        num_workers=8,
        num_classes=11,
        transform=lambda x: x,
    ) -> None:
        super().__init__()
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.train_dir = Path(train_dir) if train_dir else None
        self.val_dir = Path(val_dir) if val_dir else None
        self.name = "_".join(Path(train_dir or val_dir).parts[-3:])
        self.num_classes = num_classes

        # we can collect image stats for normalization outside
        self.transform = transform
        self.label_type = label_type
        self.bucket = bucket

    def setup(self, stage: str, rank=None, world_size=None) -> None:
        def get_label_vec(label):
            label_vec = torch.zeros(self.num_classes)
            for class_id in label:
                if class_id < self.num_classes:
                    label_vec[class_id] = 1
            return label_vec

        def setup_part(idir):
            if not idir:
                return None
            paths = get_image_paths(idir)
            if world_size and world_size > 1:
                n_per_node = ceil(len(paths) / world_size)
                paths = paths[n_per_node * rank : n_per_node * (rank + 1)]
            if self.label_type == "pkl":
                with open(idir / "labels.pkl", "rb") as f:
                    labels = pickle.load(f)
                for filename, label in labels.items():
                    labels[filename] = get_label_vec(label)
            elif self.label_type == "dir":
                ids = {category: i for i, category in enumerate(CATEGORIES)}

                def get_id(path):
                    parent = str(path.parts[-2])
                    if parent in CATEGORIES:
                        return ids[parent]
                    return int(parent)

                labels = {
                    path.name: F.one_hot(torch.tensor(get_id(path)), len(CATEGORIES))
                    for path in paths
                }
            else:
                labels = None
            dataset = ImageDataset(paths, labels, transform=self.transform)
            if self.bucket:
                with open(idir / "bucket.pkl", "rb") as f:
                    buckets = pickle.load(f)
                dataset = ImageBuckets(
                    dataset, batch_size=self.batch_size, buckets=buckets
                )
            return dataset

        self.trainset = setup_part(self.train_dir)
        self.valset = setup_part(self.val_dir)
        if not self.val_dir:
            del self.val_dataloader

    def train_dataloader(self) -> TRAIN_DATALOADERS:
        if self.bucket:
            return DataLoader(
                self.trainset,
                num_workers=self.num_workers,
                batch_size=None,  # already batched
            )
        return DataLoader(
            self.trainset, num_workers=self.num_workers, batch_size=self.batch_size
        )

    def val_dataloader(self) -> TRAIN_DATALOADERS:
        if self.bucket:
            return DataLoader(
                self.valset, num_workers=self.num_workers, batch_size=None
            )
        return DataLoader(
            self.valset, num_workers=self.num_workers, batch_size=self.batch_size
        )
