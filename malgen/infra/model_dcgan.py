import torch
from torch import nn, optim
import torchvision.datasets as dset
import torchvision.transforms as transforms
import lightning as L
from lightning.pytorch.callbacks.early_stopping import EarlyStopping

#DataModule class:
class ModelDCGAN_DataModule(L.LightningDataModule):
    # Constructor:
    def __init__(self, img_folder, batch_size=128, num_workers=2):
        self.img_folder = img_folder
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.transform = transforms.Compose(
            [
                transforms.ToTensor(),
                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
            ]
        )
        self.dl_dict = {"batch_size": self.batch_size, "num_workers": self.num_workers}

    # Get the data:
    def setup(self):
        self.data_train = dset.ImageFolder(root=self.img_folder, transform=self.transform)
        
    #Data Loaders:
    def train_dataloader(self):
        dataloader = torch.utils.data.DataLoader(self.data_train, **self.dl_dict)
        return dataloader

# Generator class for a DCGAN:
class Generator(nn.Module):
    def __init__(self, num_channels, img_size, latent_dim):
        super().__init__()
        self.model = nn.Sequential(
            nn.ConvTranspose2d(latent_dim, img_size * 8, 4, 1, 0, bias=False),
            nn.BatchNorm2d(img_size * 8),
            nn.ReLU(True),
            nn.ConvTranspose2d(img_size * 8, img_size * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(img_size * 4),
            nn.ReLU(True),
            nn.ConvTranspose2d(img_size * 4, img_size * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(img_size * 2),
            nn.ReLU(True),
            nn.ConvTranspose2d(img_size * 2, img_size, 4, 2, 1, bias=False),
            nn.BatchNorm2d(img_size),
            nn.ReLU(True),
            nn.ConvTranspose2d(img_size, num_channels, 4, 2, 1, bias=False),
            nn.Tanh(),
        )

    def forward(self, input):
        return self.model(input)


# Discriminator class for a DCGAN:
class Discriminator(nn.Module):
    def __init__(self, num_channels, img_size, leaky_slope):
        super().__init__()
        self.model = nn.Sequential(
            nn.Conv2d(num_channels, img_size, 4, 2, 1, bias=False),
            nn.LeakyReLU(leaky_slope, True),
            nn.Conv2d(img_size, img_size * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(img_size * 2),
            nn.LeakyReLU(leaky_slope, True),
            nn.Conv2d(img_size * 2, img_size * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(img_size * 4),
            nn.LeakyReLU(leaky_slope, True),
            nn.Conv2d(img_size * 4, img_size * 8, 4, 2, 1, bias=False),
            nn.BatchNorm2d(img_size * 8),
            nn.LeakyReLU(leaky_slope, True),
            nn.Conv2d(img_size * 8, 1, 4, 1, 0, bias=False),
            nn.Sigmoid(),
        )

    def forward(self, input):
        return self.model(input)

#Class for training DCGAN:
class ModelDCGAN(L.LightningModule):
    def __init__(self, num_channels, img_size, latent_dim=100, leaky_slope=0.2, lr=0.0002, beta1=0.5, beta2=0.999, batch_size=128):
        super().__init__()
        self.save_hyperparameters()

        #Create a generator
        self.generator = Generator(num_channels, img_size, latent_dim=self.hparams.latent_dim)

        #Create a discriminator
        self.discriminator = Discriminator(num_channels, img_size, leaky_slope=self.hparams.leaky_slope)

    def forward(self, input):
        return self.generator(input)

    def loss(self, expected_labels, actual_labels):
        loss_function = nn.BCELoss()
        return loss_function(expected_labels, actual_labels)

    # Training step, returns losses and progress bar info
    def training_step(self, batch, batch_idx, optimizer_idx):
        real_imgs, _ = batch
        noise = torch.randn(real_imgs.shape[0], self.hparams.latent_dim)

        # Train generator
        if optimizer_idx == 0:
            self.generated_imgs = self(noise) #SEND TO G!
            expected_labels = self.discriminator(self.generated_imgs) #SEND TO D!
            g_loss = self.loss(expected_labels, torch.ones(real_imgs.size(0), 1))
            self.log("g_loss", g_loss, prog_bar=True)
            return g_loss
        
        #Train discriminator
        if optimizer_idx == 1:
            realset_expected_labels = self.discriminator(real_imgs) #SEND TO D!
            realset_loss = self.loss(realset_expected_labels, torch.ones(real_imgs.size(0), 1))
            fakeset_expected_labels = self.discriminator(self(noise).detach()) #SEND TO D!
            fakeset_loss = self.loss(fakeset_expected_labels, torch.zeros(real_imgs.size(0), 1))
            d_loss = (realset_loss + fakeset_loss) / 2
            self.log("d_loss", d_loss, prog_bar=True)
            return d_loss
        
    #Optimizers
    def configure_optimizers(self):
        lr = self.hparams.lr
        beta1 = self.hparams.beta1
        beta2 = self.hparams.beta2
        opt_g = optim.Adam(self.generator.parameters(), lr=lr, betas=(beta1, beta2))
        opt_d = optim.Adam(self.discriminator.parameters(), lr=lr, betas=(beta1, beta2))
        return [opt_g, opt_d], [] #returns list of optimizers and empty list of schedulers (which we don't really care about for this model)
    
#EarlyStopping callback class
class ModelDCGAN_EarlyStopping(EarlyStopping):
    def on_validation_end(self, trainer, l_module):
        # override this to disable early stopping at the end of val loop
        pass

    def on_train_end(self, trainer, l_module):
        # instead, do it at the end of training loop
        self._run_early_stopping_check(trainer)


#   To create this model and train it:
#
#   early_stop_callback = EarlyStopping(monitor="g_loss", min_delta=0.00, verbose=false, patience=1, mode="min")
#   trainer = L.Trainer(max_epochs=3, callbacks=early_stop_callback)
#   model = ModelDCGAN()
#   data_module = ModelDCGAN_DataModule()
#   trainer.fit(model, data_module)
