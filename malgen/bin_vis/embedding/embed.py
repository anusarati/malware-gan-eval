"""Convert text files to embedding images"""

import sys
import numpy as np
from tqdm.contrib.concurrent import thread_map
import youtokentome as yttm
from pathlib import Path
from math import sqrt
import fasttext
import torch
from itertools import islice
import os
import argparse

model_path = "tokenizer.model"
bpe = yttm.BPE(model=model_path)
model = fasttext.load_model("malbed.bin")


def embed(filepath):
    if isinstance(filepath, str):
        filepath = filepath.rstrip()
    with open(filepath, "r") as input:
        text = input.read()
        tokens = bpe.encode(
            [text],
            output_type=yttm.OutputType.SUBWORD,
        )[0]
        # use a perfect square size for the image
        side = int(sqrt(len(tokens)))
        n = side**2
        tokens = tokens[:n]

        embeddings = np.empty((64, side, side))
        for i in range(side):
            for j in range(side):
                embeddings[:, i, j] = model[tokens[side * i + j]]

        embeddings = torch.tensor(embeddings)
        return embeddings


def save_embed(path):
    OUTDIR = Path("statout/embeds")
    outpath = OUTDIR / (Path(path).name + ".pt")
    if not os.path.isfile(outpath):
        embeddings = embed(path)
        torch.save(embeddings, outpath)


from math import ceil

if __name__ == "__main__":
    with open("rem.txt") as path_file:
        paths = path_file.readlines()

    parser = argparse.ArgumentParser()
    parser.add_argument("-rank", type=int)
    parser.add_argument("-world_size", type=int)
    args = parser.parse_args()
    print(args)

    n_per_node = ceil(len(paths) / args.world_size)
    print(n_per_node)
    start = args.rank * n_per_node
    assigned_paths = paths[start : start + n_per_node]
    print(assigned_paths)

    thread_map(
        save_embed,
        assigned_paths,
        total=min(n_per_node, len(assigned_paths) - start),
        max_workers=5,
    )
