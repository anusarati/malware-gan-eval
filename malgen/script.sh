#!/bin/bash
#SBATCH -p tiny
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=36

cd $SLURM_SUBMIT_DIR
hostname
source activate base

run_in_env() {
    local env="$1"
    shift
    local timestamp=$(date +'%Y-%m-%d %H:%M:%S')
    echo "[$timestamp] Activating environment: $env"
    conda activate "$env"
    local activation_status=$?
    if [[ $activation_status -ne 0 ]]; then
        echo "[$timestamp] ERROR: Failed to activate environment '$env'. Skipping command."
        return $activation_status
    fi
    
    echo "[$timestamp] Executing: $*"
    "$@"
    local command_status=$?
    echo "[$timestamp] Command exited with status: $command_status"
    if [[ $command_status -ne 0 ]]; then
        exit $command_status
    fi
    
    conda deactivate
    return $command_status
}


# generate images
for type in {RGB,EMBED};
do
    for size in {16,32,64,128,256,512};
    do
        model_path=progressive"$type"/$size/*/best_model.pkl
        log_path=gen"$type$size".log
        run_in_env "style" sbatch --wait --output=$log_path --error=$log_path cgen.slurm egen"$size$type" $model_path 5000
    done
done

# train classifier
train_cls() {
    type=$1
    size=$2
    log_path=egenclasslogs/$type$size.log
    run_in_env "base" sbatch --wait --output=$log_path --error=$log_path generic_gpu.slurm \
        classifier.py -mode $type --train_dir san/stylesan-xl/egen"$size$type" --label_type dir
}

for size in {256,512};
do
    train_cls L $size
done

for type in {RGB,EMBED};
do
    for size in {128,256,512};
    do
        train_cls $type $size
    done
done

echo "Script completed."