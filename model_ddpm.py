#   To install dependencies:
#   !pip install diffusers[training]==0.11.1

import torch
from torch import optim, nn
from torch.utils.data import DataLoader

import torchvision
import torchvision.transforms as transforms
import torchvision.utils as vutils
from torchvision.datasets import CIFAR10


import math
import matplotlib.pyplot as plt
import numpy as np

import lightning as L
from lightning.pytorch.callbacks.early_stopping import EarlyStopping

#DataModule class:
class ModelDCGAN_DataModule(L.LightningDataModule):
    """
    The DataModule class for ModelDCGAN. Sets up the dataset and dataloader.

    Attributes
    ----------
    data_dir : path object
        The path to the directory containing the image data.
    batch_size : int
        The size of each batch the dataloader will serve.
    num_workers : int
        The number of workers used by the dataloader.
    """

    def __init__(self, data_dir, img_size, batch_size=128, num_workers=2):
        """
        Parameters
        ----------
        data_dir : path object
            The path to the directory containing the image data.
        batch_size : int
            The size of each batch the dataloader will serve.
        num_workers : int
            The number of workers used by the dataloader.
        """
        super().__init__()
        self.data_dir = data_dir
        self.img_size = img_size
        self.batch_size = batch_size
        self.prepare_data_per_node = True
        self.num_workers = num_workers

        self.img_resize = 2**(math.ceil(math.log(img_size, 2)))
        self.transform = transforms.Compose([
                                    transforms.Resize(self.img_resize),
                                    transforms.ToTensor(),
                                    transforms.Normalize((0.5,), (0.5,))
                                ])

    def prepare_data(self):
        """
        Downloads training data from CIFAR10 if not downloaded already.
        """
        CIFAR10(self.data_dir, train=True, download=True)

    def setup(self, stage=None):
        """
        Sets up the training data from CIFAR10.
        """
        if stage == "fit" or stage is None:
            self.data_train = CIFAR10(self.data_dir, train=True, transform=self.transform)

    def train_dataloader(self):
        """
        Creates the DataLoader for ModelDCGAN.
        """
        dataloader = DataLoader(self.data_train, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True)
        return dataloader

#UNet class for a DDPM
class UNet(nn.Module):
    def __init__(self, num_channels, img_size, layers_per_block):
        super().__init__()
        self.model = UNet2DModel(
            sample_size=img_size,
            in_channels=num_channels,
            out_channels=num_channels,
            layers_per_block=layers_per_block,
            block_out_channels=(
                num_channels, num_channels, 
                num_channels*2, num_channels*2, 
                num_channels*4, num_channels*4
            ),
            down_block_types=(
                "DownBlock2D",
                "DownBlock2D",
                "DownBlock2D",
                "DownBlock2D",
                "AttnDownBlock2D",
                "DownBlock2D",
            ),
            up_block_types=(
                "UpBlock2D",
                "AttnUpBlock2D",
                "UpBlock2D",
                "UpBlock2D",
                "UpBlock2D",
                "UpBlock2D"
            ),
        )
    
    #This DDPM takes the timestep as another parameter
    def forward(self, input, timestep):
        return self.model(input, timestep)

#Class for training DDPM
class ModelDDPM(L.LightningModule):
    def __init__(self, num_channels, img_size, num_timesteps, layers_per_block=2, lr=2e-5, beta1=0.9, beta2=0.999, batch_size=128):
        super().__init__()
        self.save_hyperparameters()
        self.epoch_number = 0
        #Create a noise scheduler:
        self.noise_scheduler = DDPMScheduler(num_timesteps)

        #Create a UNet:
        self.unet = UNet(num_channels, img_size, layers_per_block)

        self.control_noise = torch.randn(batch_size)


    def forward(self, input, timestep):
        return self.unet(input, timestep)
    
    def loss(self, expected_noise, actual_noise):
        loss_function = nn.MSELoss() #Seems like MSE was used in original paper
        return loss_function(expected_noise, actual_noise)
    
    def training_step(self, batch, batch_idx):
        real_imgs,_ = batch
        if self.epoch_number == 0:
            self.control_imgs = real_imgs
            
        #Sample noise to add to images
        noise = torch.randn(real_imgs.shape[0])
        #Sample a random timestep for each image
        timesteps = torch.randint(0, self.hparam.num_timesteps, real_imgs.shape[0])
        #Add noise to real_imgs according to noise magnitude at each timestep
        noisy_imgs = self.noise_scheduler.add_noise(real_imgs, noise, timesteps)
        expected_noise = self(noisy_imgs, timesteps) #SEND TO UNET!
        unet_loss = self.loss(expected_noise, noise)
        self.log("unet_loss", unet_loss, prog_bar=True)
        return unet_loss

    def on_train_epoch_end(self):
        if (self.epoch_number > 0) and (self.epoch_number % 2 == 0):
            generated_control = self(self.control_noise)
            fig = plt.figure(figsize=(10,10))
            plt.axis("off")
            plt.title("Generated Images After {} Epochs".format(self.epoch_number))
            grid = vutils.make_grid(generated_control[:40], padding=2, normalize=True)
            plt.imshow(np.transpose(grid, (1, 2, 0)))
            plt.savefig("g_images_{}.png".format(self.epoch_number))
            plt.close(fig)
        self.epoch_number += 1

    def configure_optimizers(self):
        lr = self.hparams.lr
        beta1 = self.hparams.beta1
        beta2 = self.hparams.beta2
        optimizer = torch.optim.Adam(self.unet.parameters(), lr=lr, betas=(beta1, beta2))
        #Check to see if it's better to use a CosineAnnealingLR scheduler
        return [optimizer], [] #returns list of optimizers and empty list of schedulers 

#EarlyStopping callback class
class ModelDDPM_EarlyStopping(EarlyStopping):
    def on_validation_end(self, trainer, l_module):
        # override this to disable early stopping at the end of val loop
        pass

    def on_train_end(self, trainer, l_module):
        # instead, do it at the end of training loop
        self._run_early_stopping_check(trainer)

#   To create this model and train it:
#
#   early_stop_callback = EarlyStopping(monitor="unet_loss", min_delta=0.00, verbose=false, patience=1, mode="min")
#   trainer = L.Trainer(max_epochs=3, callbacks=early_stop_callback)
#   model = ModelDDPM()
#   data_module = ModelDDPM_DataModule()
#   trainer.fit(model, data_module)