#   To install dependencies:
#   !pip install diffusers[training]==0.11.1

import torch
from torch import nn, optim
import torchvision.datasets as dset
import torchvision.transforms as transforms
import lightning as L
from lightning.pytorch.callbacks.early_stopping import EarlyStopping
from diffusers import UNet2DModel, DDPMScheduler

#DataModule class:
class ModelDDPM_DataModule(L.LightningDataModule):
    #Constructor:
    def __init__(self, img_folder, batch_size=128, num_workers=2):
        self.img_folder = img_folder
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.transform = transforms.Compose([
                                    transforms.ToTensor(),
                                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
                                ])
        self.dl_dict = {'batch_size': self.batch_size, 'num_workers': self.num_workers}

    #Get the data:
    def setup(self):
        self.data_train = dset.ImageFolder(root=self.img_folder, transform=self.transform)

    #Data Loaders:
    def train_dataloader(self):
        dataloader = torch.utils.data.DataLoader(self.data_train, **self.dl_dict)
        return dataloader

#UNet class for a DDPM
class UNet(nn.Module):
    def __init__(self, num_channels, img_size, layers_per_block):
        super().__init__()
        self.model = UNet2DModel(
            sample_size=img_size,
            in_channels=num_channels,
            out_channels=num_channels,
            layers_per_block=layers_per_block,
            block_out_channels=(
                num_channels, num_channels, 
                num_channels*2, num_channels*2, 
                num_channels*4, num_channels*4
            ),
            down_block_types=(
                "DownBlock2D",
                "DownBlock2D",
                "DownBlock2D",
                "DownBlock2D",
                "AttnDownBlock2D",
                "DownBlock2D",
            ),
            up_block_types=(
                "UpBlock2D",
                "AttnUpBlock2D",
                "UpBlock2D",
                "UpBlock2D",
                "UpBlock2D",
                "UpBlock2D"
            ),
        )
    
    #This DDPM takes the timestep as another parameter
    def forward(self, input, timestep):
        return self.model(input, timestep)

#Class for training DDPM
class ModelDDPM(L.LightningModule):
    def __init__(self, num_channels, img_size, num_timesteps, layers_per_block=2, lr=2e-5, beta1=0.9, beta2=0.999, batch_size=128):
        super().__init__()
        self.save_hyperparameters()

        #Create a noise scheduler:
        self.noise_scheduler = DDPMScheduler(num_timesteps)

        #Create a UNet:
        self.unet = UNet(num_channels, img_size, layers_per_block)

    def forward(self, input, timestep):
        return self.unet(input, timestep)
    
    def loss(self, expected_noise, actual_noise):
        loss_function = nn.MSELoss() #Seems like MSE was used in original paper
        return loss_function(expected_noise, actual_noise)
    
    def training_step(self, batch, batch_idx):
        real_imgs,_ = batch
        #Sample noise to add to images
        noise = torch.randn(real_imgs.shape[0])
        #Sample a random timestep for each image
        timesteps = torch.randint(0, self.hparam.num_timesteps, real_imgs.shape[0])
        #Add noise to real_imgs according to noise magnitude at each timestep
        noisy_imgs = self.noise_scheduler.add_noise(real_imgs, noise, timesteps)
        expected_noise = self(noisy_imgs, timesteps) #SEND TO UNET!
        unet_loss = self.loss(expected_noise, noise)
        self.log("unet_loss", unet_loss, prog_bar=True)
        return unet_loss

    def configure_optimizers(self):
        lr = self.hparams.lr
        beta1 = self.hparams.beta1
        beta2 = self.hparams.beta2
        optimizer = torch.optim.Adam(self.unet.parameters(), lr=lr, betas=(beta1, beta2))
        #Check to see if it's better to use a CosineAnnealingLR scheduler
        return [optimizer], [] #returns list of optimizers and empty list of schedulers 

#EarlyStopping callback class
class ModelDDPM_EarlyStopping(EarlyStopping):
    def on_validation_end(self, trainer, l_module):
        # override this to disable early stopping at the end of val loop
        pass

    def on_train_end(self, trainer, l_module):
        # instead, do it at the end of training loop
        self._run_early_stopping_check(trainer)

#   To create this model and train it:
#
#   early_stop_callback = EarlyStopping(monitor="unet_loss", min_delta=0.00, verbose=false, patience=1, mode="min")
#   trainer = L.Trainer(max_epochs=3, callbacks=early_stop_callback)
#   model = ModelDDPM()
#   data_module = ModelDDPM_DataModule()
#   trainer.fit(model, data_module)