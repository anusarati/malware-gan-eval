import torch
from torch import optim, nn
from torch.utils.data import DataLoader

import torchvision
import torchvision.transforms as transforms
import torchvision.utils as vutils
from torchvision.datasets import MNIST


import math
import matplotlib.pyplot as plt
import numpy as np

import lightning as L

from transformers import ConvNextV2Model

class EvalClassifier_DataModule(L.LightningDataModule):
    """
    The DataModule class for EvalClassifier. Sets up the dataset and dataloader.

    Attributes
    ----------
    data_dir : path object
        The path to the directory containing the image data.
    batch_size : int
        The size of each batch the dataloader will serve.
    num_workers : int
        The number of workers used by the dataloader.
    """

    def __init__(self, data_dir, img_size, batch_size=128, num_workers=2):
        """
        Parameters
        ----------
        data_dir : path object
            The path to the directory containing the image data.
        batch_size : int
            The size of each batch the dataloader will serve.
        num_workers : int
            The number of workers used by the dataloader.
        """
        super().__init__()
        self.data_dir = data_dir
        self.img_size = img_size
        self.batch_size = batch_size
        self.prepare_data_per_node = True
        self.num_workers = num_workers

        self.img_resize = 2**(math.ceil(math.log(img_size, 2)))
        self.transform = transforms.Compose([
                                    transforms.Resize(self.img_resize),
                                    transforms.ToTensor(),
                                    transforms.Normalize((0.5,), (0.5,))
                                ])

    def prepare_data(self):
        """
        Downloads training data from MNIST if not downloaded already.
        """
        MNIST(self.data_dir, train=True, download=True)
        # MNIST(self.data_dir, train=False, download=True)

    def setup(self, stage=None):
        """
        Sets up the training data from MNIST.
        """
        if stage == "fit" or stage is None:
            self.data_train = MNIST(self.data_dir, train=True, transform=self.transform)
            # self.data_test = CIFAR10(self.data_dir, train=False, transform=self.transform)

    def train_dataloader(self):
        """
        Creates the DataLoader for EvalClassifier.
        """
        dataloader = DataLoader(self.data_train, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True)
        return dataloader
    
    def test_dataloader(self):
        dataloader = DataLoader(self.data_test, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True)
        return dataloader

class ConvNext(nn.Module):
        def __init__(self, num_channels, img_size):
            super().__init__()
            self.model = ConvNextV2Model(num_channels=num_channels, image_size=img_size)

        def forward(self, images):
             output = self.model(images)
             return output
        
class EvalClassifier(L.LightningModule):
     
    def __init__(self, num_channels, img_size, lr=0.001, beta1=0.5, beta2=0.999):
        super().__init__()
        self.save_hyperparameters()
        self.convnext = ConvNext(num_channels, img_size)
        self.automatic_optimization = False

    def forward(self, images):
        return self.convnext(images)
    
    def loss(self, expected_labels, actual_labels):
        loss_function = nn.CrossEntropyLoss()
        return loss_function(expected_labels, actual_labels)
    
    def training_step(self, batch, batch_idx):
        images, actual_labels = batch
        opt = self.optimizers()

        opt.zero_grad()
        expected_labels = self(images)
        train_loss = self.loss(expected_labels, actual_labels)
        self.manual_backward(train_loss)
        opt.step()
        self.log("train_loss", train_loss, prog_bar=True)

    # def test_step(self, batch, batch_idx):
    #     images, actual_labels = batch
    #     expected_labels = self(images)
    #     test_loss = self.loss(expected_labels, actual_labels)
    #     self.log("test_loss", test_loss, prog_bar=True)
    #     test_acc = torch.sum(expected_labels == actual_labels).item() / (len(actual_labels) * 1.0)
    #     self.log("test_acc", test_acc, prog_bar=True)

    def configure_optimizers(self):
        lr = self.hparams.lr
        beta1 = self.hparams.beta1
        beta2 = self.hparams.beta2
        opt = optim.AdamW(self.convnext.parameters(), lr=lr, betas=(beta1, beta2))
        return opt


          
             


#use an convnext model!!
#CIFAR10 classes (in order): ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']


