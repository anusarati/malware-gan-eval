"""Train model to generate embeddings for byte sequences"""

import torch
from torch import optim, nn
from torch.utils.data import Dataset, DataLoader
from torch.nn.parameter import Parameter
import torch.nn.functional as F
from transformers import AutoModelForMaskedLM, BitsAndBytesConfig
from lightning.pytorch.plugins import BitsandbytesPrecision
import lightning as L
from lightning.pytorch.callbacks import ModelCheckpoint
import pandas as pd
from mlm_pytorch import MLM
from pathlib import Path
import random
from math import sqrt
import os
from tqdm import tqdm
from peft import LoraConfig, get_peft_model
from math import ceil
import bitsandbytes as bnb

# in a 512x512 image, each pixel needs to encode 38 bytes to represent a 10 MB binary


class Embedder(L.LightningModule):
    def __init__(self, model):
        super().__init__()
        if isinstance(model, Embedder):
            self.model = model.model
            self.mlm_trainer = model.mlm_trainer
        else:
            self.model = model
            self.mlm_trainer = MLM(
                transformer=lambda x: model(x).logits,
                mask_token_id=256,
                pad_token_id=257,
                mask_prob=0.15,
                replace_prob=0.90,
                mask_ignore_token_ids=[],
            )
        
    def training_step(self, batch, batch_idx):
        loss = self.mlm_trainer(batch)
        self.log("loss", loss, prog_bar=True)
        self.log("mem", torch.cuda.max_memory_allocated(), prog_bar=True)
        torch.cuda.reset_peak_memory_stats()
        return loss
    
    def forward(self, batch):
        return self.model(batch)

    def configure_optimizers(self):
        optimizer = bnb.optim.Adam8bit(self.parameters())

        # https://lightning.ai/docs/pytorch/stable/common/precision_intermediate.html#quantization-via-bitsandbytes
        # (optional) force embedding layers to use 32 bit for numerical stability
        # https://github.com/huggingface/transformers/issues/14819#issuecomment-1003445038
        for module in self.modules():
            if isinstance(module, nn.Embedding):
                bnb.optim.GlobalOptimManager.get_instance().register_module_override(
                    module, "weight", {"optim_bits": 32}
                )

        return optimizer

    @torch.inference_mode()
    def embed(self, path, seq_len):
        """
        Returns embeddings for each consecutive seq_len byte sequence in a file
        truncated to the nearest square multiple of seq_len bytes
        """
        with open(path, "rb", buffering=0) as f:
            bin = f.readall()
            encoder = self.model.base_model
            size = os.path.getsize(path)
            n = int(sqrt(size / seq_len)) ** 2
            bins = (bin[seq_len * i : seq_len * (i + 1)] for i in range(n))
            inputs = (
                torch.frombuffer(bin, dtype=torch.uint8)
                .to(self.device, dtype=torch.int32)
                .unsqueeze(0)
                for bin in bins
            )
            MAX_BATCH = 4096
            batch_sizes = (
                min(MAX_BATCH, n - MAX_BATCH * i) for i in range(ceil(n / MAX_BATCH))
            )
            batches = (
                torch.cat(tuple(next(inputs) for _ in range(batch_size)))
                for batch_size in batch_sizes
            )
            embeds = tuple(
                encoder(batch).last_hidden_state.mean(1) for batch in batches
            )
            embeds = torch.cat(embeds)
            assert n == embeds.shape[0], (n, embeds.shape)
            # (n, 768)
            return embeds

    @torch.inference_mode()
    def embed_image(self, path, seq_len=64):
        """Returns an image of embeddings for a file"""
        emb = self.embed(path, seq_len)
        n = emb.shape[0]
        features = emb.shape[1]
        side = int(sqrt(n))
        assert side * side == n, f"{side}**2 == {side**2} != {n}"
        # convert features to channels
        emb = emb.T.reshape(features, side, side)
        # (C, s, s)
        return emb


class BinaryDataset(Dataset):
    def __init__(self, dir, seq_len):
        super().__init__()
        self.paths = list(Path(dir).iterdir())
        self.seq_len = seq_len

    def __len__(self):
        return len(self.paths)

    def __getitem__(self, idx):
        """Returns up to 128 random sequences of seq_len bytes from a file"""
        path = self.paths[idx]
        with open(path, "rb", buffering=0) as f:
            bin = f.readall()
            sequences = tuple(
                torch.tensor(list(bin[p : p + self.seq_len]))
                for p in [random.randrange(len(bin) - self.seq_len) for _ in range(128)]
            )
            return torch.stack(sequences)


def surgery(model, vocab_size):
    """Shrinks the number of input/output tokens of the model"""
    special = model.base_model
    word_embeddings = special.embeddings.word_embeddings
    word_embeddings.weight = Parameter(word_embeddings.weight[:vocab_size, :])
    word_embeddings.num_embeddings = vocab_size
    decoder = model.cls.predictions.decoder
    decoder.weight = Parameter(decoder.weight[:vocab_size, :])
    decoder.bias = Parameter(decoder.bias[:vocab_size])
    decoder.out_features = vocab_size


def get_fresh_pretrained():
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_quant_storage=torch.bfloat16,
    )

    model = AutoModelForMaskedLM.from_pretrained(
        "microsoft/xtremedistil-l6-h256-uncased",
        quantization_config=bnb_config,
        torch_dtype=torch.bfloat16,
    )

    surgery(model, 258)
    lora_config = LoraConfig(
        task_type="FEATURE_EXTRACTION",
        r=8,
        lora_alpha=32,
        lora_dropout=0.01,
        target_modules="all-linear",
    )
    model = get_peft_model(model, lora_config)
    embedder = Embedder(model)
    return embedder


# embedder = Embedder.load_from_checkpoint("goodies/512_80000.ckpt", model=get_fresh_pretrained(), strict=False)
embedder = get_fresh_pretrained()


def train(seq_len):
    dataset = BinaryDataset("bins/decompressed", seq_len)
    dataloader = DataLoader(
        dataset, num_workers=64, prefetch_factor=10, collate_fn=torch.cat, batch_size=2
    )

    EPOCHS = 2
    torch.set_float32_matmul_precision("medium")
    precision = BitsandbytesPrecision(mode="nf4")

    trainer = L.Trainer(
        devices=-1,
        max_epochs=EPOCHS,
        callbacks=ModelCheckpoint(
            save_on_train_epoch_end=True, save_last=True, every_n_train_steps=5000
        ),
        strategy="ddp_find_unused_parameters_true",
        plugins=precision,
    )

    print(trainer.predict(embedder, dataloader))
    return

    trainer.fit(
        model=embedder,
        train_dataloaders=dataloader,
    )


class ChannelReducer(nn.Module):
    """Reduces number of channels using PCA"""

    def __init__(self, data, n):
        super().__init__()
        _, _, self.V = torch.pca_lowrank(data, n)

    def __call__(self, img):
        # img : (channel, height, width)
        # (height, width, channel) @ (channel, n)
        return (img.permute(1, 2, 0) @ self.V).permute(2, 0, 1)


def save_reducers():
    import pickle
    from tqdm import tqdm
    import cProfile

    meta: pd.DataFrame = pd.read_pickle("corrected.pkl")
    torch.set_float32_matmul_precision("medium")
    embedder.eval()
    embedder.bfloat16()

    with torch.inference_mode():

        def sample_embeds(n):
            data = []
            sample = meta.sample(n)
            example = f"bins/decompressed/{sample.iloc[0]['sha256']}"

            cProfile.runctx(
                "embedder.embed_image(example)",
                {"embedder": embedder, "example": example},
                {},
                sort="cumulative",
            )

            with tqdm(
                sample.iterrows(),
                total=sample.shape[0],
            ) as progress:
                for _, row in progress:
                    torch.cuda.reset_max_memory_allocated()
                    path = f"bins/decompressed/{row['sha256']}"
                    data.append(embedder.embed_image(path))
                    progress.set_postfix(mem=torch.cuda.max_memory_allocated())

            return data

        data = sample_embeds(5000)

        def save_reducer(data):
            # (n, C)
            data = torch.cat(list(map(lambda x: x.flatten(1), data)), dim=1).T

            reducer = ChannelReducer(data, 64)
            print("Reducer err:", F.mse_loss(data @ reducer.V @ reducer.V.T, data))
            print("Avg abs value:", data.abs().mean())
            pickle.dump(reducer, open("reducer.pkl", "wb"))
            return reducer

        def save_rgb_reducer(reducer, data):
            data = map(reducer, data)
            data = torch.cat(list(map(lambda x: x.flatten(1), data)), dim=1).T
            rgb_reducer = ChannelReducer(data, 3)
            print(
                "RGB reducer err:",
                F.mse_loss(data @ rgb_reducer.V @ rgb_reducer.V.T, data),
            )
            print("Avg abs value:", data.abs().mean())
            pickle.dump(rgb_reducer, open("rgb_reducer.pkl", "wb"))
            return rgb_reducer

        reducer = save_reducer(data)
        save_rgb_reducer(reducer, data)


if __name__ == "__main__":
    train(512)
